{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91494514-f883-42d5-b987-890c19e7645b",
   "metadata": {},
   "source": [
    "# GOALS\n",
    "\n",
    "## What needs to be decided on?\n",
    "\n",
    "- Split feature\n",
    "- Split point\n",
    "- When to stop splitting\n",
    "\n",
    "# Steps\n",
    "\n",
    "## Pre-testing\n",
    "\n",
    "1. Calculate **information gain (IG)** with each possible split\n",
    "2. Divide set with that feature and value that gives the most IG\n",
    "3. Divide tree and do the same for all created branches...\n",
    "4. Repeat Steps 1-3 until a **stopping criteria** is reached.\n",
    "\n",
    "\n",
    "## Testing\n",
    "\n",
    "Given a data point:\n",
    "- Follow the tree until you reach a leaf node.\n",
    "\n",
    "    - If the leaf is PURE, we return the class of the leaf node. \n",
    "    - If the leaf is NOT PURE, we return the most common class label **(Majority Vote Method)**\n",
    "\n",
    "# Terms\n",
    "\n",
    "**IG = (Entropy of Parent) - [Weighted Average * Entropy of Children]**\n",
    "\n",
    "Entropy - unordered\n",
    "\n",
    " - Root (Leaf Node) - The entropy is 0 since it is the last and therefore ordered.\n",
    " \n",
    "Stopping criteria: maximum depth, minimum number of samples, min impurity decrease\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7f63fb4-3979-4f00-9ae1-4de18dd86039",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initial Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75e32e13-6138-4ef1-a8c3-9eeca5f8581b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature=None,\n",
    "                 threshold=None, left = None, \n",
    "                 right = None, *, value = None):\n",
    "        self.feature = feature  #which feature this was divide with\n",
    "        self.threshold = threshold #which threshold this was divided with\n",
    "        self.left = left #the left tree and left node it is pointing to\n",
    "        self.right = right #the right tree and right node it is pointing to\n",
    "        self.value= None \n",
    "\n",
    "    #* means to call the function need to include the explicit Name\n",
    "    #i.e. Node.value\n",
    "    \n",
    "    \n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None #value exists it means it is a leaf node\n",
    "    \n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split = 2, max_depth = 100, n_features = None):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.n_features = n_features\n",
    "        self.root = None\n",
    "        \n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1], self.n_features)\n",
    "            #return X.shape[1] (or the number of features)  if self.n_features is not defined\n",
    "            #OR return the values from the range X.shape[1] and self.n_features\n",
    "            \n",
    "            #Checks so that \"no. of features IS NOT more than actual no. of features\"\n",
    "            \n",
    "        self.root = self._grow_tree(X, y) #Return the root of the tree in\n",
    "        \n",
    "    def _grow_tree(self, X, y, depth = 0 ):\n",
    "        n_samples, n_feats = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "        \n",
    "        #check the stopping criteria\n",
    "        if (depth >= self.max_depth or n_labels == 1 or n_samples<self.min_samples_split):\n",
    "            #starts at 0 and increase as we create child nodes, we increase it by 1\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "        \n",
    "     #if depth >= max_depth = 0\n",
    "        #OR\n",
    "        #n_labels == 1 is a leaf node\n",
    "        #OR\n",
    "        #n_samples is less than minimum sample split\n",
    "        \n",
    "        #RETURN\n",
    "        \n",
    "        \n",
    "        feat_idx = np.ramdom.choice(n_feats, self.n_features, replace = False) \n",
    "        #use numpy random choice.\n",
    "            # Pass number of features or n_feats\n",
    "            #Number of features we want to select from our objects or self.n_features\n",
    "            #replace by default is True. \n",
    "    \n",
    "        #find the best split\n",
    "        best_feature, best_thresh = self._best_split(X, y, feat_idx)\n",
    "        \n",
    "    def _best_split(self, X, y, feat_idxs): #Find best threshold among all possible threshold; and find the best split among all possible splits\n",
    "        best_gain = -1\n",
    "        split_idx, split_threshold = None, None\n",
    "        \n",
    "        for feat_idx, in feat_idxs:\n",
    "            X_column = X[:, feat_idx]\n",
    "            thresholds =  np.unique(X_column)\n",
    "            \n",
    "            #traverse again\n",
    "            for thr in thresholds: #for all threshold (thr) in thresholds \n",
    "                gain = self._information_gain(y, X_column, thr)\n",
    "                \n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_threshold = thr\n",
    "                    \n",
    "        return split_idx, split_threshold\n",
    "    \n",
    "    #Main calculation for IG\n",
    "    pass\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        value = counter.most_common(1)[0][0]\n",
    "        return value\n",
    "    \n",
    "    def predict():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63a41c7-57ee-4d5d-91be-c60d858e5eb0",
   "metadata": {},
   "source": [
    "# Add the IG formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cce74076-a3ed-48c7-bf86-8c7a61ddcae9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature=None,\n",
    "                 threshold=None, left = None, \n",
    "                 right = None, *, value = None):\n",
    "        self.feature = feature  \n",
    "        self.threshold = threshold \n",
    "        self.left = left \n",
    "        self.right = right \n",
    "        self.value= None \n",
    "    \n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None \n",
    "    \n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split = 2, max_depth = 100, n_features = None):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.n_features = n_features\n",
    "        self.root = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1], self.n_features)   \n",
    "        self.root = self._grow_tree(X, y) \n",
    "        \n",
    "    def _grow_tree(self, X, y, depth = 0 ):\n",
    "        n_samples, n_feats = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "        \n",
    "\n",
    "        if (depth >= self.max_depth or n_labels == 1 or n_samples<self.min_samples_split):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        feat_idx = np.ramdom.choice(n_feats, self.n_features, replace = False) \n",
    "        best_feature, best_thresh = self._best_split(X, y, feat_idx)\n",
    "        \n",
    "    def _best_split(self, X, y, feat_idxs):\n",
    "        best_gain = -1\n",
    "        split_idx, split_threshold = None, None\n",
    "        \n",
    "        for feat_idx, in feat_idxs:\n",
    "            X_column = X[:, feat_idx]\n",
    "            thresholds =  np.unique(X_column)\n",
    "            \n",
    "            for thr in thresholds: \n",
    "                gain = self._information_gain(y, X_column, thr)\n",
    "                \n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_threshold = thr\n",
    "                    \n",
    "        return split_idx, split_threshold\n",
    "    \n",
    "    #Main calculation for IG\n",
    "    \n",
    "    def _information_gain(self, y, X_column, threshold):\n",
    "        #parent entropy\n",
    "        parent_entropy = self._entropy(y)\n",
    "        \n",
    "        #create children\n",
    "        left_idx, right_idx = self._split(X_column, threshold)\n",
    "        \n",
    "        if len(left_idx) == 0 or len(right_idx) == 0: #if the left OR right indices is empty\n",
    "            return 0 \n",
    "        \n",
    "        #calculate the weighted avg. entropy of children\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs) #no. of samples in left, no. of samples in right\n",
    "        e_l, e_r = self.entropy(y[left_idxs]), self.entropy(y[right_idxs]) #entrop of left, entrop of right\n",
    "        \n",
    "        #child entropy is (no. of samples /total no.of samples) * (entropy of children) THEREFORE\n",
    "        child_entropy = ( (n_l/n) * e_l ) + ( (n_r/n) * e_r )\n",
    "        \n",
    "        #calculate the IG\n",
    "        information_gain = parent_entropy - child_entropy\n",
    "        return information_gain\n",
    "        \n",
    "    def _split(self, X_column, split_thresh):\n",
    "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
    "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "    \n",
    "    def _entropy(self, y):\n",
    "        hist = np.bincount(y)\n",
    "        p_x = hist/len(y) #p(X)\n",
    "        \n",
    "        return -np.sum([p*np.log(p) for p in p_x if p>0]) #Again, why doesn't she add the 2 for log2?\n",
    "        \n",
    "        \n",
    "        #create child nodes\n",
    "        pass\n",
    "    \n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        value = counter.most_common(1)[0][0]\n",
    "        return value\n",
    "    \n",
    "    def predict():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4e83011-8778-43c3-9090-1703e75318a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#illustrating numpy bincount\n",
    "y = np.array([1,2,3,1,2])\n",
    "np.bincount(y)\n",
    "\n",
    "#starts at zero. 0 has occured zero times, 1 has occured 2 times\n",
    "#... 3 has occured 1 time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "601cf2cd-af6a-492f-ba0c-8320a91d826c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [3],\n",
       "       [4]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#numpy arg where\n",
    "y = np.array([1,2,3,1,2])\n",
    "\n",
    "np.argwhere(y<3) #it skipped 3 which was the index = 2 position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d56e6d6d-3c79-4c42-b6db-929cc0564e9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 3, 4], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(y<3).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c5f359-52ac-4f43-8a88-b2912c37bcb4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d07a96de-85f7-48ff-8241-fba6a44f5f27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature=None,\n",
    "                 threshold=None, left = None, \n",
    "                 right = None, *, value = None):\n",
    "        self.feature = feature  \n",
    "        self.threshold = threshold \n",
    "        self.left = left \n",
    "        self.right = right \n",
    "        self.value= value\n",
    "    \n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None \n",
    "    \n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split = 2, max_depth = 100, n_features = None):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.n_features = n_features\n",
    "        self.root = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1], self.n_features)   \n",
    "        self.root = self._grow_tree(X, y) \n",
    "        \n",
    "    def _grow_tree(self, X, y, depth = 0 ):\n",
    "        n_samples, n_feats = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "        \n",
    "        #check the stopping criteria\n",
    "        if (depth >= self.max_depth or n_labels == 1 or n_samples<self.min_samples_split):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "        \n",
    "        feat_idx = np.random.choice(n_feats, self.n_features, replace = False) \n",
    "        \n",
    "        #find the best split\n",
    "        best_feature, best_thresh = self._best_split(X, y, feat_idx) \n",
    "        \n",
    "        # CODE PATH\n",
    "        #Pass the arguement(X, y, feat_idx)  and load _best_split\n",
    "            #Best split loads and calls information gain\n",
    "                #information gain loads and calls _entropy and _split\n",
    "                    #_entropy returns entropy value\n",
    "                    #_split returns left_idxs, right_idxs\n",
    "            #backtrack to information gain and return  information_gain = parent_entropy - child_entropy\n",
    "            \n",
    "        #Backtrack to _best_split and the IG is stored in gain\n",
    "            #if gain > best_gain (which starts at -1)\n",
    "                #updated best_gain with gain\n",
    "                #split_idx = feat_idx\n",
    "                #split_threshold = thr\n",
    "        #best_feature = split_idx, best_thresh = split_threshold\n",
    "        \n",
    "        #From there create the Child Nodes\n",
    "        \n",
    "        #create child nodes\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)\n",
    "        left = self._grow_tree( X[left_idxs, :], y[left_idxs], depth+1) #Increase depth by 1 OR current depth + 1\n",
    "        right = self._grow_tree( X[right_idxs, :], y[right_idxs], depth+1) #Increase depth by 1 OR current depth + 1\n",
    "        return Node(best_feature, best_thresh, left, right)\n",
    "        \n",
    "    def _best_split(self, X, y, feat_idxs):\n",
    "        best_gain = -1\n",
    "        split_idx, split_threshold = None, None\n",
    "        \n",
    "        for feat_idx in feat_idxs:\n",
    "            X_column = X[:, feat_idx]\n",
    "            thresholds =  np.unique(X_column)\n",
    "            \n",
    "            for thr in thresholds: \n",
    "                gain = self._information_gain(y, X_column, thr)\n",
    "                \n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_threshold = thr\n",
    "                    \n",
    "        return split_idx, split_threshold\n",
    "    \n",
    "    #Main calculation for IG\n",
    "    \n",
    "    def _information_gain(self, y, X_column, threshold):\n",
    "        #parent entropy\n",
    "        parent_entropy = self._entropy(y)\n",
    "        \n",
    "        #create children\n",
    "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
    "        \n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0: #if the left OR right indices is empty\n",
    "            return 0 \n",
    "        \n",
    "        #calculate the weighted avg. entropy of children\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs) #no. of samples in left, no. of samples in right\n",
    "        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs]) #entrop of left, entrop of right\n",
    "        \n",
    "        #child entropy is (no. of samples /total no.of samples) * (entropy of children) THEREFORE\n",
    "        child_entropy = ( (n_l/n) * e_l ) + ( (n_r/n) * e_r )\n",
    "        \n",
    "        #calculate the IG\n",
    "        information_gain = parent_entropy - child_entropy\n",
    "        return information_gain\n",
    "        \n",
    "    def _split(self, X_column, split_thresh):\n",
    "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
    "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "    \n",
    "    def _entropy(self, y):\n",
    "        hist = np.bincount(y)\n",
    "        p_x = hist/len(y) #p(X)\n",
    "        \n",
    "        return -np.sum([p*np.log(p) for p in p_x if p>0]) #Again, why doesn't she add the 2 for log2?\n",
    "    \n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        value = counter.most_common(1)[0][0]\n",
    "        return value\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "        \n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node(): #I'm assuming class object name is cases insensitive so Node.is_leaf or node. is_leaf\n",
    "            return node.value\n",
    "        \n",
    "        #left, right contains a certain \"feature we divided it with\" \n",
    "        #X[left_idxs, :]\n",
    "        \n",
    "        #x[node.feature] = feature we divided it with\n",
    "        \n",
    "        \n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left) #if less, we traverse on the left\n",
    "        return self._traverse_tree(x, node.right) #else, we traverse right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84912562-d5ed-4c31-a899-8f3d78bd97c1",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49d57c6-54a1-4f67-88d5-33588852b2aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "#from DecisionTree import DecisionTree\n",
    "\n",
    "data = datasets.load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state = 1234\n",
    ")\n",
    "\n",
    "clf = DecisionTree()\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "def accuracy(y_test, y_pred):\n",
    "    return np.sum(y_test == y_pred) / len(y_test)\n",
    "\n",
    "acc = accuracy(y_test, predictions)\n",
    "print(acc)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2305fe2c-fe4c-4e85-85ec-fe2d95244769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we change it to max_depth = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72193dd-5aa4-43db-b29c-0c5768299214",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "#from DecisionTree import DecisionTree\n",
    "\n",
    "data = datasets.load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state = 1234\n",
    ")\n",
    "\n",
    "clf = DecisionTree(max_depth = 10) #CHANGE HERE\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "def accuracy(y_test, y_pred):\n",
    "    return np.sum(y_test == y_pred) / len(y_test)\n",
    "\n",
    "acc = accuracy(y_test, predictions)\n",
    "print(acc)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
