{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b04ae84f-591d-4ef7-8ddc-2cea6ebe3ffc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# GOAL\n",
    "\n",
    "Find Majority Vote - AMONG multiple decisions trees.\n",
    "\n",
    "\n",
    "\n",
    "# Steps\n",
    "\n",
    "## Pre-testing\n",
    "\n",
    "Given the whole dataset:\n",
    "- Get a subset of the dataset\n",
    "- Create a decision tree\n",
    "- Repeat for as many times as the number of trees\n",
    "\n",
    "For each **Decision Tree...**\n",
    "\n",
    "### What needs to be decided on?\n",
    "\n",
    "- Split feature\n",
    "- Split point\n",
    "- When to stop splitting\n",
    "\n",
    "1. Calculate **information gain (IG)** with each possible split\n",
    "2. Divide set with that feature and value that gives the most IG\n",
    "3. Divide tree and do the same for all created branches...\n",
    "4. Repeat Steps 1-3 until a **stopping criteria** is reached.\n",
    "\n",
    "\n",
    "## Testing\n",
    "\n",
    "### Given a data point:\n",
    "- Follow the tree until you reach a leaf node.\n",
    "\n",
    "    - If the leaf is PURE, we return the class of the leaf node. \n",
    "    - If the leaf is NOT PURE, we return the most common class label **(Majority Vote Method)**\n",
    "\n",
    "### Finally\n",
    "\n",
    "- Get the predictions from each tree\n",
    "- *Classification:* hold a majority vote\n",
    "- *Regression:* get the mean of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "309c01f9-820f-430b-951e-2dbeaa685d46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run Decision Tree Code\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature=None,\n",
    "                 threshold=None, left = None, \n",
    "                 right = None, *, value = None):\n",
    "        self.feature = feature  \n",
    "        self.threshold = threshold \n",
    "        self.left = left \n",
    "        self.right = right \n",
    "        self.value= value\n",
    "    \n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None \n",
    "    \n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split = 2, max_depth = 100, n_features = None):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.n_features = n_features\n",
    "        self.root = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1], self.n_features)   \n",
    "        self.root = self._grow_tree(X, y) \n",
    "        \n",
    "    def _grow_tree(self, X, y, depth = 0 ):\n",
    "        n_samples, n_feats = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "        \n",
    "        #check the stopping criteria\n",
    "        if (depth >= self.max_depth or n_labels == 1 or n_samples<self.min_samples_split):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "        \n",
    "        feat_idx = np.random.choice(n_feats, self.n_features, replace = False) \n",
    "        \n",
    "        #find the best split\n",
    "        best_feature, best_thresh = self._best_split(X, y, feat_idx) \n",
    "        \n",
    "        # CODE PATH\n",
    "        #Pass the arguement(X, y, feat_idx)  and load _best_split\n",
    "            #Best split loads and calls information gain\n",
    "                #information gain loads and calls _entropy and _split\n",
    "                    #_entropy returns entropy value\n",
    "                    #_split returns left_idxs, right_idxs\n",
    "            #backtrack to information gain and return  information_gain = parent_entropy - child_entropy\n",
    "            \n",
    "        #Backtrack to _best_split and the IG is stored in gain\n",
    "            #if gain > best_gain (which starts at -1)\n",
    "                #updated best_gain with gain\n",
    "                #split_idx = feat_idx\n",
    "                #split_threshold = thr\n",
    "        #best_feature = split_idx, best_thresh = split_threshold\n",
    "        \n",
    "        #From there create the Child Nodes\n",
    "        \n",
    "        #create child nodes\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)\n",
    "        left = self._grow_tree( X[left_idxs, :], y[left_idxs], depth+1) #Increase depth by 1 OR current depth + 1\n",
    "        right = self._grow_tree( X[right_idxs, :], y[right_idxs], depth+1) #Increase depth by 1 OR current depth + 1\n",
    "        return Node(best_feature, best_thresh, left, right)\n",
    "        \n",
    "    def _best_split(self, X, y, feat_idxs):\n",
    "        best_gain = -1\n",
    "        split_idx, split_threshold = None, None\n",
    "        \n",
    "        for feat_idx in feat_idxs:\n",
    "            X_column = X[:, feat_idx]\n",
    "            thresholds =  np.unique(X_column)\n",
    "            \n",
    "            for thr in thresholds: \n",
    "                gain = self._information_gain(y, X_column, thr)\n",
    "                \n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_threshold = thr\n",
    "                    \n",
    "        return split_idx, split_threshold\n",
    "    \n",
    "    #Main calculation for IG\n",
    "    \n",
    "    def _information_gain(self, y, X_column, threshold):\n",
    "        #parent entropy\n",
    "        parent_entropy = self._entropy(y)\n",
    "        \n",
    "        #create children\n",
    "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
    "        \n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0: #if the left OR right indices is empty\n",
    "            return 0 \n",
    "        \n",
    "        #calculate the weighted avg. entropy of children\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs) #no. of samples in left, no. of samples in right\n",
    "        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs]) #entropy of left, entropy of right\n",
    "        \n",
    "        #child entropy is (no. of samples /total no.of samples) * (entropy of children) THEREFORE\n",
    "        child_entropy = ( (n_l/n) * e_l ) + ( (n_r/n) * e_r )\n",
    "        \n",
    "        #calculate the IG\n",
    "        information_gain = parent_entropy - child_entropy\n",
    "        return information_gain\n",
    "        \n",
    "    def _split(self, X_column, split_thresh):\n",
    "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
    "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "    \n",
    "    def _entropy(self, y):\n",
    "        hist = np.bincount(y)\n",
    "        p_x = hist/len(y) #p(X)\n",
    "        \n",
    "        return -np.sum([p*np.log(p) for p in p_x if p>0]) #Again, why doesn't she add the 2 for log2?\n",
    "    \n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        value = counter.most_common(1)[0][0]\n",
    "        return value\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "        \n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node(): #I'm assuming class object name is cases insensitive so Node.is_leaf or node. is_leaf\n",
    "            return node.value\n",
    "        \n",
    "        #left, right contains a certain \"feature we divided it with\" \n",
    "        #X[left_idxs, :]\n",
    "        \n",
    "        #x[node.feature] = feature we divided it with\n",
    "        \n",
    "        \n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left) #if less, we traverse on the left\n",
    "        return self._traverse_tree(x, node.right) #else, we traverse right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a17d9b8c-59c7-4908-a2e0-beeb7709060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intial Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "628d76b7-df97-4140-a620-3e381689d94f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as no\n",
    "from collections import Counter\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_trees=10, max_depth =10, min_samples_split = 2, n_features = None): #initial values chosen at random\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.n_features = n_features\n",
    "        self.trees = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        for _ in range(self.n_trees):\n",
    "            \n",
    "            tree = DecisionTree(max_depth = self.max_depth,\n",
    "                        min_samples_split = self.min_samples_split,\n",
    "                        n_features = self.n_features)\n",
    "            \n",
    "            X_sample, y_sample = self._bootstrap_samples(X,y) #Helper function 1\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "    def _bootstrap_samples(self, X, y):\n",
    "        n_samples = X.shape[0] #In a numpy array, first index is number of samples\n",
    "                                # second index is number of features\n",
    "            \n",
    "        idxs = np.random.choice(n_samples, n_samples, replace = True) #After a sample is selected in the dataset\n",
    "                                                                      #some samples would be dropped, some samples will be used AGAIN\n",
    "        return X[idxs], y[idxs]\n",
    "    \n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        most_common = counter.most_common(1)[0][0] #Gets the most common occurence of a certain value from a reconstructed array.\n",
    "        return most_common\n",
    "        \n",
    "    def predict(self, X):\n",
    "        predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        \n",
    "            #result of predictions -- \"a list of lists\"\n",
    "            # One list per tree --> [ [1,0,1,1],[0,0,1,1],[] ]\n",
    "                #First Tree: 1 is the first sample, 0 second sample etc.\n",
    "            #\" A list of lists\" --> [[1,0]]\n",
    "                #1 is the prediction for the 1st tree, 0 is the prediction of the 2nd tree\n",
    "            \n",
    "        tree_preds = np.swapaxes(predictions, 0, 1)\n",
    "        predictions = np.array([self._most_common_label(pred) for pred in tree_preds])\n",
    "        return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dea998-780c-4ac9-b59f-732503185313",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b87e6978-255b-4216-9e32-f0efbfb8f3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9035087719298246\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "#from RandomForest import RandomForest\n",
    "\n",
    "data = datasets.load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.2, random_state = 1234\n",
    ")\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred) /  len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "clf = RandomForest()\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "acc = accuracy(y_test, predictions)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1272fdb4-ac7d-451c-979d-fc9334947805",
   "metadata": {},
   "source": [
    "# Change values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e492c270-8005-41a0-9556-3d67797f2dff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9122807017543859\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "#from RandomForest import RandomForest\n",
    "\n",
    "data = datasets.load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.2, random_state = 1234\n",
    ")\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred) /  len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "clf = RandomForest(n_trees = 20)\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "acc = accuracy(y_test, predictions)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53cc1cf2-fad5-4197-a157-f7f99914eac0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "#from RandomForest import RandomForest\n",
    "\n",
    "data = datasets.load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.2, random_state = 1234\n",
    ")\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred) /  len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "clf = RandomForest(n_trees = 20, max_depth = 5)\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "acc = accuracy(y_test, predictions)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73995c98-2655-44d9-9fb0-c275a7c7171c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
